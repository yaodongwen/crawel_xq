import gzip
import json
import time
import re
from lxml import etree  # å¿…é¡»å¯¼å…¥
import config
from DrissionPage import ChromiumPage, ChromiumOptions
import time
import threading
import os
import config
from db_manager import DBManager
from spider_tools import SpiderTools

class SpiderPortfolioMixin:
    # ================= Step 3: æ‰¹æ¬¡çˆ¬å– (å«é•¿æ–‡é€»è¾‘) =================
    def __init__(self):
        self.driver = self._init_browser()

    def _init_browser(self):
        co = ChromiumOptions()
        co.set_browser_path(config.MAC_CHROME_PATH)
        co.set_user_data_path(config.USER_DATA_PATH)
        co.set_local_port(9337) 
        co.set_argument('--ignore-certificate-errors')
        try: return ChromiumPage(co)
        except Exception as e: 
            print(f"\n[å¯åŠ¨é”™è¯¯] {e}"); exit()
    
    # ================= ä¼˜åŒ–åçš„è¯„è®ºè·å–é€»è¾‘ =================
    def _parse_comments_fragment(self, html_content):
        """
        æ ¸å¿ƒæå–é€»è¾‘ï¼šè§£æç›‘å¬åˆ°çš„ HTML ç‰‡æ®µ
        """
        # å¦‚æœ body æ˜¯ bytes ç±»å‹ï¼Œå…ˆè§£ç 
        if isinstance(html_content, bytes):
            html_text = html_content.decode('utf-8', errors='ignore')
        else:
            html_text = html_content

        tree = etree.HTML(html_text)
        # è·å–æ‰€æœ‰åŠ¨æ€æ¡ç›®
        items = tree.xpath('//div[contains(@class, "status-item")]')
        results = []

        for item in items:
            try:
                # 1. æå–ä½œè€…å (å¯¹åº”ä½ æˆªå›¾ä¸­çš„ï¼š96èˆ¹ç¥¨_)
                # è·¯å¾„å®šä½åˆ° status-bd ä¸‹çš„ status-retweet-user é‡Œçš„ a æ ‡ç­¾
                author = item.xpath('.//div[@class="status-retweet-user"]/a[@class="name"]/text()')
                author_name = author[0].strip() if author else "æœªçŸ¥ä½œè€…"

                # 2. æå–æ­£æ–‡ (text é‡Œçš„æ‰€æœ‰æ–‡å­—)
                content_nodes1 = item.xpath('.//div[@class="text"]//text()')
                content_nodes2 = item.xpath('.//script[@class="single-description"]//text()')
                content_nodes = content_nodes2 if len(content_nodes1) < len(content_nodes2) else content_nodes1
                content = content_nodes

                # 3. æå–äº’åŠ¨æ•° (ç‚¹èµå’Œè®¨è®º)
                likes = item.xpath('.//a[contains(@class, "btn-like")]//em/text()')
                comments = item.xpath('.//a[contains(@class, "btn-status-reply")]//em/text()')

                results.append({
                    "author": author_name,
                    "text": content,
                    "likes": likes[0] if likes else "0",
                    "comments": comments[0] if comments else "0"
                })
            except Exception:
                continue
        return results
    
    def _portfolio_status(self, symbol, tab):
            
            # 2. è®¿é—®é¡µé¢
            url = f"https://xueqiu.com/P/{symbol}"
            
            # å‡è®¾ tab æ˜¯å½“å‰æ ‡ç­¾é¡µå¯¹è±¡
            cube_closed = tab.ele('xpath://div[@class="cube-closed"]')

            if cube_closed:
                # è·å– .text ä¸‹çš„ä¸¤ä¸ª p æ ‡ç­¾
                p_elements = cube_closed.eles('xpath:.//div[@class="text"]/p')
                
                create_time = None
                close_time = None
                
                for p in p_elements:
                    text = p.text.strip()
                    if 'åˆ›å»ºäº' in text:
                        create_time = text.replace('åˆ›å»ºäºï¼š', '').strip()
                    elif 'å…³åœæ—¶é—´' in text:
                        close_time = text.replace('å…³åœæ—¶é—´ï¼š', '').strip()
                
                print(f"åˆ›å»ºæ—¶é—´: {create_time}")
                print(f"å…³åœæ—¶é—´: {close_time}")
            else:
                print("ç»„åˆå¼€å¯ä¸­")

    def _mine_portfolio(self, symbol):
        """
        ç»„åˆè¯¦æƒ…è·å–é€»è¾‘ï¼š
        ç›´æ¥æ–°å»ºæ ‡ç­¾é¡µè®¿é—®ç»„åˆè¯¦æƒ…é¡µ URL (https://xueqiu.com/P/{symbol})ï¼Œ
        æŠ“å–å®Œæ•´ç»„åˆä¿¡æ¯åè¿”å›ã€‚
        """
        try:
            # æ„é€ é•¿æ–‡é“¾æ¥
            url = f"https://xueqiu.com/P/{symbol}"

            # æ‰“å¼€æ–°æ ‡ç­¾é¡µ (DrissionPage ä¼šè‡ªåŠ¨åˆ‡æ¢ç„¦ç‚¹åˆ°æ–°é¡µé¢)
            detail_tab = self.driver.new_tab(url)

            # ç­‰å¾…æ ¸å¿ƒå…ƒç´ åŠ è½½ (æ ‡é¢˜æˆ–æ­£æ–‡)
            # ç»™ 5 ç§’è¶…æ—¶ï¼Œé˜²æ­¢é¡µé¢åŠ è½½å¤ªæ…¢å¡ä½
            title_ele = detail_tab.ele('.cube-title', timeout=5)  # æ³¨æ„ï¼šclass æ˜¯ cube-titleï¼Œä¸æ˜¯ article__bd__title

            # è·å– ç»„åˆåå’Œå…³æ³¨æ•° 
            name_text = title_ele.ele('.name').text
            xpath = '//div[@class="cube-title"]//div[@class="cube-people-data"]//span[@class="num"]'
            follows_span = detail_tab.ele('xpath:' + xpath)
            follows_num = re.search(r'(\d+)', follows_span.text).group(1)  # å¾—åˆ° '103'

            # è·å–ç›ˆåˆ©æ•°æ®
            info_container = detail_tab.ele('#cube-info', timeout=5)
            # è·å–æ‰€æœ‰ per ç±»çš„ span
            per_spans = info_container.eles('.per')

            # éå†å¹¶æ‰“å°æ¯ä¸ªå€¼ åˆ†åˆ«æ˜¯æ€»æ”¶ç›Šï¼Œæ—¥ï¼Œæœˆï¼Œå‡€å€¼ï¼Œæ€»æ”¶ç›Šæ’è¡Œè¶…è¿‡%
            for i, span in enumerate(per_spans):
                print(f"ç¬¬{i+1}ä¸ª per å€¼: {span.text}")

            # è·å–ç”¨æˆ·ä¿¡æ¯
            # å®šä½æ•´ä¸ª creator-info åŒºåŸŸï¼ˆå¯é€‰ï¼Œç”¨äºé™å®šèŒƒå›´ï¼‰
            creator_info = detail_tab.ele('xpath://div[contains(@class, "cube-creator-info")]')

            # 1. è·å– IDï¼šä» creator é“¾æ¥çš„ href ä¸­æå–
            href = detail_tab.ele('xpath://div[contains(@class, "cube-creator-info")]//a[contains(@class, "creator")]', timeout=5).attr('href')
            user_id = href.strip('/').split('/')[-1]  # å¾—åˆ° "1433550277"

            # 2. è·å–ç”¨æˆ·åï¼šåœ¨ creator ä¸‹çš„ .name
            name = detail_tab.ele('xpath://div[contains(@class, "cube-creator-info")]//a[contains(@class, "creator")]//div[@class="name"]').text

            # 3. è·å–æè¿°ï¼šåœ¨ desc > span.text
            desc = detail_tab.ele('xpath://div[contains(@class, "cube-creator-info")]//div[@class="desc"]/span[@class="text"]').text

            print(f"ID: {user_id}")
            print(f"åç§°: {name}")
            print(f"æè¿°: {desc}")


            # è·å–ä»“ä½ä¿¡æ¯
            # è·å–æ‰€æœ‰ stock <a> æ ‡ç­¾ï¼ˆä½¿ç”¨ XPathï¼‰
            stock_names = detail_tab.eles('xpath://div[@class="weight-list"]//div[contains(@class, "segment")]')
            for stocks in stock_names:
                stock_name = detail_tab.ele('xpath://div[@class="weight-list"]//span[@class="segment-name"]').text
                stock_num = detail_tab.ele('xpath://div[@class="weight-list"]//span[@class="segment-weight weight"]').text
                print(stock_name)
                print(stock_num)

                stock_elements = detail_tab.eles('xpath://div[@class="weight-list"]//a[contains(@class, "stock")]')

                for stock in stock_elements:
                    name = stock.ele('xpath:.//div[@class="name"]').text
                    price = stock.ele('xpath:.//div[@class="price"]').text
                    weight = stock.ele('xpath:.//span[contains(@class, "stock-weight")]').text

                    print(f"{name} | {price} | {weight}")

            # è·å– è¯„è®º
            # åœ¨ detail_tab ä¸­æ‰§è¡Œä¸€æ®µ JSï¼Œä¸€æ¬¡æ€§æå–æ‰€æœ‰åŠ¨æ€æ•°æ®
            # 1. è®¾ç½®ç›‘å¬
            self.driver.listen.start('cube/timeline')
            
            # 2. è®¿é—®é¡µé¢
            url = f"https://xueqiu.com/P/{symbol}"
            self.driver.get(url)
            
            # 3. è§¦å‘åŠ è½½ (å‘ä¸‹æ»šåŠ¨)
            self.driver.scroll.down(1000)
            
            # 4. è·å–æ‹¦æˆªåˆ°çš„æ•°æ®åŒ…
            res = self.driver.listen.wait(timeout=5)
            if res:
                # æ‹¿åˆ°æ¥å£è¿”å›çš„æ··åˆ HTML æ–‡æœ¬
                comments = self._parse_comments_fragment(res.response.body)
                
                for c in comments:
                    print(f"ã€{c['author']}ã€‘: {c['text'][:50]}...")
                    print(f"   ğŸ“Š èµ: {c['likes']} | è®¨è®º: {c['comments']}")
                    print("-" * 40)
            else:
                print("âŒ æœªæ•è·åˆ° timeline æ¥å£æ•°æ®")



            # è·å–å†å²è°ƒä»“
            res_rebalances = self._mine_rebalance(symbol,detail_tab)
            print(res_rebalances)

            # æŠ“å–å®Œæˆåå…³é—­å½“å‰é•¿æ–‡é¡µ
            detail_tab.close()


        except Exception as e:
            print(f"    âš ï¸ ç»„åˆè·å–å¤±è´¥ {symbol}: {e}")
            # å¼‚å¸¸ä¿æŠ¤ï¼šå¦‚æœæ ‡ç­¾é¡µæ²¡å…³æ‰ï¼Œå¼ºåˆ¶å…³é—­
            if self.driver.tabs_count > 1:
                # ç®€å•åˆ¤æ–­ä¸€ä¸‹å½“å‰é¡µæ˜¯ä¸æ˜¯åˆ—è¡¨é¡µï¼Œå¦‚æœä¸æ˜¯å°±å…³æ‰
                if str(symbol) not in self.driver.latest_tab.url:
                    self.driver.latest_tab.close()
            return None
    

    def _mine_rebalance(self, symbol, tab):
        try:
            url = f"https://xueqiu.com/P/{symbol}"
            # tab = self.driver.new_tab(url)
            print(f"å·²æ‰“å¼€ç»„åˆé¡µ: {symbol}")

            # ç›‘å¬è°ƒä»“æ¥å£
            tab.listen.start('rebalancing/history.json')
            tab.get(url)

            btn = tab.ele('xpath://a[@class="history"]')
            if btn:
                btn.click(by_js=True)
            # ç­‰å¾…è¯·æ±‚ï¼ˆnew_tab å·²åŠ è½½é¡µé¢ï¼Œç›´æ¥ç­‰å³å¯ï¼‰
            res = tab.listen.wait(timeout=3)
            data = SpiderTools.decode_response(res)

            if data is None:
                print(f"âŒ {symbol}: æœªæ•è·åˆ°è°ƒä»“è®°å½•æ¥å£")
                return None


        except Exception as e:
            print(f"âš ï¸ {symbol} å‡ºé”™: {e}")
            return None
        finally:
            if 'tab' in locals():
                tab.close()

        return data
    

aa = SpiderPortfolioMixin()
aa._mine_portfolio('ZH3084474')
